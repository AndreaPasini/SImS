"""
This file contains functions related to the PRS computation and analysis:
- create_PRS() get image training graphs and compute PRS (triplets and histograms)
"""

import json
import os
from multiprocessing.pool import Pool
import pyximport
pyximport.install(language_level=3)
from scipy.stats import entropy
from tqdm import tqdm
from sims.graph_utils import json_to_nx, nx_to_json


def compute_hist_from_graph(graph):
    """
    Compute histogram from a scene graph
    :param graph input json graph
    :return: histogram
    """
    # Create map with node labels
    node_labels = {n['id'] : n['label'] for n in graph['nodes']}
    hist = {}
    for l in graph['links']:
        s = node_labels[l['s']]
        r = node_labels[l['r']]
        pair = f"{s},{r}"
        pos = l['pos']
        if pair in hist:
            h_sr = hist[pair]
            if pos in h_sr:
                h_sr[pos]+=1
            else:
                h_sr[pos]=1
        else:
            hist[pair] = {}
            hist[pair][pos]=1
    return hist

def create_PRS(graphs_json_path, PRS_dir, out_json_path):
    """
    Analyze scene graphs generated by create_scene_graphs()
    Generate PRS histograms
    :param graphs_json_path: json file with graphs
    :param PRS_dir: output PRS directory (same dir of out_json_path)
    :param out_json_path: output file path
    """
    print("Extracting PRS...")
    if not os.path.exists(PRS_dir):
        os.makedirs(PRS_dir)

    with open(graphs_json_path, "r") as f:
        graphs = json.load(f)

    # Init progress bar
    pbar = tqdm(total=len(graphs))

    def update(x):
        pbar.update()

    print("Number of scene graphs: %d" % len(graphs))
    print("Scheduling tasks...")
    pool = Pool(10)
    results = []

    # Analyze all graphs
    for graph in graphs:
        results.append(pool.apply_async(compute_hist_from_graph, args=(graph,), callback=update))
    pool.close()
    pool.join()
    pbar.close()

    print("Collecting results...")
    # Collect histogram results
    histograms = {}
    for res_getter in results:
        h_pairs = res_getter.get()
        if h_pairs is not None:
            for pair, hist in h_pairs.items():
                if pair not in histograms:
                    # add histogram as it is if pair is not existing
                    histograms[pair] = hist
                else:
                    total_hist = histograms[pair]
                    # update histograms if pair already existing
                    for key in hist:
                        if key in total_hist:
                            total_hist[key] += hist[key]
                        else:
                            total_hist[key] = hist[key]

    n_pairs = 0
    # Compute histogram supports and entropy
    for hist in histograms.values():
        sup = sum(hist.values())  # support: sum of all occurrences in the histogram
        ent = []
        for pos, count in hist.items():
            perc = count / sup
            hist[pos] = perc
            ent.append(perc)
        hist['sup'] = sup
        n_pairs += sup
        # Important: the dictionary inside hist may not contain all the different position labels
        # E.g. {'side':0.5, 'side-up':0.5}
        # There are missing zeros like ... 'above':0, 'on':0,...
        # However these 0 terms does not influence entropy (because 0*log(0)=0)
        hist['entropy'] = entropy(ent, base=2)

    with open(out_json_path, "w") as f:
        json.dump({str(k): v for k, v in histograms.items()}, f)
    print("Done")
    print(f"Number of analyzed object pairs: {n_pairs}")
    print(f"Number of generated histograms: {len(histograms)}")

def filter_PRS_histograms(prs, min_sup, max_entropy):
    """
    :param prs: pairwise relationship summary (read from json)
    :param min_sup: minimum support for filtering histograms
    :param max_entropy: maximum entropy for filtering histograms
    :return: filtered PRS, based on min_sup and max_entropy
    """
    return {pair : h for pair, h in prs.items() if h['sup'] >= min_sup and h['entropy'] <= max_entropy}

def get_likelihood(nodes_map, link, prs):
    """
    Retrieve from the PRS the likelihood of this link
    :param nodes_map: map NodeId:ObjectClassLabel
    :param link: graph link, dictionary {s,pos,r}
    :param prs: pairwise relationship summary (read from json)
    :return: likelihood l of link and histogram; None if there is no histogram
    """
    sub = nodes_map[link['s']]
    ref = nodes_map[link['r']]
    pos = link['pos']
    pair = f"{sub},{ref}"

    # Check for the likelihood in the PRS
    if pair in prs:
        hist = prs[f"{sub},{ref}"]
        if pos in hist:
            return hist[pos], hist
        else:
            return 0, hist
    return None, None


def get_sup_ent_lists(prs):
    """
    Get support and entropy values in two lists, from the PRS (json format)
    :param prs: pairwise relationship summary (read from json)
    """
    sup = [h['sup'] for h in prs.values()]
    ent = [h['entropy'] for h in prs.values()]
    return sup, ent


def edge_pruning(prs, graphs):
    """
    Prune graph edges, when they are not present in the PRS
    :param prs: pairwise relationship summary (read from json)
    :param graphs: list of json scene graphs
    :return: pruned graphs
    """
    stat_avg_nlinks = 0
    stat_avg_nlinks_filtered = 0
    pruned_graphs = []
    for g in graphs:
        nodes_map = {node['id'] : node['label'] for node in g['nodes']}
        links = []
        for link in g['links']:
            l,h = get_likelihood(nodes_map, link, prs)
            if l is not None:
                links.append(link)
        if len(links)!=len(g['links']):
            in_edge_ids = []
            for link in links:
                in_edge_ids.append(link['s'])
                in_edge_ids.append(link['r'])
            in_edge_ids = set(in_edge_ids)
            nodes_list = []
            for n in g['nodes']:
                if n['id'] in in_edge_ids:
                    nodes_list.append(n)
        else:
            nodes_list = g['nodes']
        pruned_graph = {'directed':g['directed'],'multigraph':g['multigraph'],'graph':g['graph'],'nodes': nodes_list, 'links':links}
        pruned_graphs.append(pruned_graph)
        # Update statistics
        stat_avg_nlinks += len(g['links'])
        stat_avg_nlinks_filtered += len(pruned_graph['links'])

    print(f"Average number of links in scene graphs: {stat_avg_nlinks/len(graphs)}")
    print(f"Average number of links in pruned graphs: {stat_avg_nlinks_filtered / len(graphs)}")

    return pruned_graphs

def describe_node_edges(graph, node, nodes_map):
    """
    Given the node id, describe its inbound and outbound edges with edge and node labels.
    Can be used to find equivalent objects (that are related in the same way with other objects in the image).
    :param graph: input json graph
    :param node: id of the node to be described
    :param nodes_map: map with node_id:node_label
    :return: a list of tuples - (node_label, edge_label) for inbound, (edge_label, node_label) for outbound edges
    """
    inedges = list(graph.in_edges(node, data=True))
    outedges = list(graph.out_edges(node, data=True))
    description = [(e[2]['pos'], nodes_map[e[1]]) for e in outedges]
    description += [(nodes_map[e[0]], e[2]['pos']) for e in inedges]
    return set(description)


def node_pruning(graphs):
    """
    Prune graph nodes, when they are equivalent according to describe_node_edges() result.
    Two equivalent objects are related in the same way with other objects in the image.
    They can be considered as single object while performing graph mining
    :param: input json scene graphs
    :return: pruned graphs
    """
    stat_avg_nnodes = 0
    stat_avg_nnodes_filtered = 0
    pruned_graphs = []
    for g in graphs:
        grouped_nodes = {}
        g_nx = json_to_nx(g)
        nodes_map = {node['id']: node['label'] for node in g['nodes']}
        # Describe each node with inbound and outbound edges
        nodes_description = {node['id']: describe_node_edges(g_nx, node['id'], nodes_map) for node in g['nodes']}
        # Group nodes by class
        for node in g['nodes']:
            if node['label'] in grouped_nodes:
                grouped_nodes[node['label']].append(node['id'])
            else:
                grouped_nodes[node['label']] = [node['id']]
        # For each group of nodes of the same class
        for label, group in grouped_nodes.items():
            while len(group)>1: # Do until it contains more than 1 elements (that could be merged)
                a = group.pop()  # Remove the first element
                for b in group.copy(): # Compare a with all the other elements
                    if nodes_description[a]==nodes_description[b]:
                        # b is equal to a (same inbound and outbound edges), it can be removed
                        group.remove(b)
                        g_nx.remove_node(b)
        pruned_graph = nx_to_json(g_nx)
        pruned_graphs.append(pruned_graph)
        stat_avg_nnodes += len(nodes_map)
        stat_avg_nnodes_filtered += len(pruned_graph['nodes'])
    print(f"Average number of nodes in graphs: {stat_avg_nnodes / len(graphs)}")
    print(f"Average number of nodes in pruned graphs: {stat_avg_nnodes_filtered / len(graphs)}")
    print(f"Number of removed nodes: {stat_avg_nnodes - stat_avg_nnodes_filtered} ({100*(stat_avg_nnodes - stat_avg_nnodes_filtered)/stat_avg_nnodes}%)")
    return pruned_graphs